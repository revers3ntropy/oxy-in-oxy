def test_lexer (tr: TestRunner) {
	tr.it("tokenises a single integer", fn (tr: TestRunner) {
		let program = "12"
		let lexer_res = Lexer.lex(program, "lexer-test.oxy")
		tr.expect_ok!<List<Token>>(lexer_res)
		let tokens = lexer_res.unwrap()
		tr.expect!<Int>(tokens.len()).to_be(1)
		tr.expect!<Int>(tokens.at_raw(0).token_type).to_be(TT_INT)
	})

	tr.it("tokenises multiple integers", fn (tr: TestRunner) {
		let program = "12 3 6789"
		let lexer_res = Lexer.lex(program, "lexer-test.oxy")
		tr.expect_ok!<List<Token>>(lexer_res)
		let tokens = lexer_res.unwrap()
		tr.expect!<Int>(tokens.len()).to_be(3)
		tr.expect!<Int>(tokens.at_raw(0).token_type).to_be(TT_INT)
		tr.expect!<Int>(tokens.at_raw(1).token_type).to_be(TT_INT)
		tr.expect!<Int>(tokens.at_raw(2).token_type).to_be(TT_INT)
	})

	tr.it("tokenises a string", fn (tr: TestRunner) {
		let program = "\"hi\""
		let tokens = Lexer.lex(program, "lexer-test.oxy").unwrap()
		tr.expect!<Int>(tokens.len()).to_be(1)
		tr.expect!<Int>(tokens.at_raw(0).token_type).to_be(TT_STRING)
	})

	tr.it("tokenises a character literal", fn (tr: TestRunner) {
		let tokens = Lexer.lex("'a'", "lexer-test.oxy").unwrap()
		tr.expect!<Int>(tokens.len()).to_be(1)
		tr.expect!<Int>(tokens.at_raw(0).token_type).to_be(TT_CHAR)
	})

	tr.it("does not tokenise an invalid character literal", fn (tr: TestRunner) {
		let lexer_res = Lexer.lex("'abc'", "lexer-test.oxy")
		tr.expect!<Bool>(lexer_res.ok).to_be(false)
	})

	tr.it("ignores comments but does not ignore tokens before or after", fn (tr: TestRunner) {
		let program = "12 // comment\n 'b'"
		let tokens = Lexer.lex(program, "lexer-test.oxy").unwrap()
		tr.expect!<Int>(tokens.len()).to_be(2)
		tr.expect!<Int>(tokens.at_raw(0).token_type).to_be(TT_INT)
		tr.expect!<Int>(tokens.at_raw(1).token_type).to_be(TT_CHAR)
	})
}